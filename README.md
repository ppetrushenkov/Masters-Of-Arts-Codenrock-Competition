# Masters of Arts: ML Challenge
Задача Masters of Arts: ML Challenge — необходимо определить вид произведения искусства по фотографии: скульптура, живопись, графика, декоративно-прикладное искусство, инсталляция или архитектура. Главное настроить алгоритм распознавания более точно. В качестве метрики выбран mean Average Precision (mAP).

# Пайплан решения

## Каркас
В работе был задействован фреймворк `pytorch-lightning`, который позволяет настроить процесс обработки данных, а также обучения моделей наиболее удобным способом. Так же данный фреймворк включает в себя такие функции для обучения моделей как накопление градиента, стохастичестическое усредние весов моделей и многое другое, все функции которого работают 'прямо из коробки'.

Сначала был создан датамодуль (`ArtDataModule`), находящийся в `dataset.py`. Так же в нем находится класс датасета (`ArtDataset`), который обрабатывает папку с изображениями.

Датамодуль позволяет удобным образом настроить обработку данных, какой будет размер батча и какие будут трансформации для изображений. Так же в данном датамодуле добавлен метод `WeightRandomSampler`, дающий больший вес тем классам, которых меньше в датасете. Это используется при имбалансе классов. 

## Модель
Для решения задачи классификации использовались модели `Vit` (Visual Transformer) и `RegNet`, которые обучались по принципу Transfer learning. Последний слой был изменен на несколько линейных, параметры которых и изменялись в дальнейшем.

## Улучшения
Для повышения качества модели применялись методы которые были взяты от 
<a href="https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/">сюда</a>.

## Аугментация
В данной работе были применены следующие аугментации:

* TrivialAugmentWide
* AugMix
* Random Erasing (p=0.1)

Эти аугментации применялись случайным способом на каждой итерации. 

## Дополнительные методы
* Learning rate finder (pytorch-lightning находит наилучший lr для текущей ситуации)
* Long training (30 эпох)
* Label Smoothing (0.1)
* StochasticWeightAveraging (1e-3)

# Итоги
Все приведенные выше улучшения повысили точность модели с 0.89 до 0.9262.